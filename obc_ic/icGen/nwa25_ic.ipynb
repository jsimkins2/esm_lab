{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0373e7e1-c048-4256-a1c4-3beb9ef8aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray\n",
    "import xesmf\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "from dask.base import tokenize\n",
    "import dask.array as dsa\n",
    "import xarray as xr\n",
    "import dask \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Use SODA data centered on 1992-12-30.\n",
    "    # Model start date is 1993-01-01.\n",
    "    # https://dsrs.atmos.umd.edu/DATA/soda3.12.2/REGRIDED/ocean/soda3.12.2_5dy_ocean_reg_1993_01_04.nc\n",
    "    #soda_file = f'/glade/scratch/jsimkins/SODA3.12.2/soda3.12.2_5dy_ocean_reg_1993_01_04.nc'\n",
    "    soda_file = '/home/james/SODA/5day/soda3.3.1_5dy_ocean_reg_2010_01_05.nc'\n",
    "    \n",
    "    #start_date = np.datetime64('1993-01-02T00:00:00')\n",
    "    start_date = np.datetime64('2010-01-01T00:00:00')\n",
    "    # Used in filename below, don't change\n",
    "    start_str = np.datetime_as_string(start_date, unit='D')\n",
    "    \n",
    "    # Save the ICs here:\n",
    "    #output_file = f'/glade/u/home/jsimkins/obc_ic/nwa25/nwa25_soda_ic_75z_{start_str}.nc'\n",
    "    output_file = f'/home/james/initCond/nwa25/nwa25_soda_ic_75z_{start_str}.nc'\n",
    "    \n",
    "    # Model vertical grid:\n",
    "    #vgrid_file = '/glade/u/home/jsimkins/obc_ic/nwa25/vgrid_zl.nc'\n",
    "    vgrid_file = '/home/james/gridInfo/nwa25/vgrid_zl.nc'\n",
    "    \n",
    "    # Model horizontal grid:\n",
    "    #grid_file = '/glade/u/home/jsimkins/obc_ic/nwa25/nwa25_ocean_hgrid.nc'\n",
    "    grid_file = '/home/james/gridInfo/nwa25/nwa25_ocean_hgrid.nc'\n",
    "    \n",
    "    write_initial(soda_file, vgrid_file, grid_file, start_date, output_file)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# https://github.com/raphaeldussin/HCtFlood\n",
    "# code courtesy of Raphael Dussin - https://github.com/raphaeldussin/HCtFlood/blob/master/HCtFlood/kara.py\n",
    "\n",
    "def flood_kara(data, xdim='lon', ydim='lat', zdim='z', tdim='time',\n",
    "               spval=1e+15):\n",
    "    \"\"\"Apply extrapolation onto land from Kara algo.\n",
    "    Arguments:\n",
    "        data {xarray.DataArray} -- input data\n",
    "    Keyword Arguments:\n",
    "        xdim {str} -- name of x dimension (default: {'lon'})\n",
    "        ydim {str} -- name of y dimension (default: {'lat'})\n",
    "        zdim {str} -- name of z dimension (default: {'z'})\n",
    "        tdim {str} -- name of time dimension (default: {'time'})\n",
    "        spval {float} -- missing value (default: {1e+15})\n",
    "    Returns:\n",
    "        xarray.DataArray -- result of the extrapolation\n",
    "    \"\"\"\n",
    "    # check for input data shape\n",
    "    if tdim not in data.dims:\n",
    "        data = data.expand_dims(dim=tdim)\n",
    "    if zdim not in data.dims:\n",
    "        data = data.expand_dims(dim=zdim)\n",
    "\n",
    "    nrec = len(data[tdim])\n",
    "    nlev = len(data[zdim])\n",
    "    ny = len(data[ydim])\n",
    "    nx = len(data[xdim])\n",
    "    shape = (nrec, nlev, ny, nx)\n",
    "    chunks = (1, 1, ny, nx)\n",
    "\n",
    "    def compute_chunk(zlev, trec):\n",
    "        data_slice = data.isel({tdim: trec, zdim: zlev})\n",
    "        return flood_kara_xr(data_slice, spval=spval)[None, None]\n",
    "\n",
    "    name = str(data.name) + '-' + tokenize(data.name, shape)\n",
    "    dsk = {(name, rec, lev, 0, 0,): (compute_chunk, lev, rec)\n",
    "           for lev in range(nlev)\n",
    "           for rec in range(nrec)}\n",
    "\n",
    "    out = dsa.Array(dsk, name, chunks,\n",
    "                    dtype=data.dtype, shape=shape)\n",
    "\n",
    "    xout = xr.DataArray(data=out, name=str(data.name),\n",
    "                        coords={tdim: data[tdim],\n",
    "                                zdim: data[zdim],\n",
    "                                ydim: data[ydim],\n",
    "                                xdim: data[xdim]},\n",
    "                        dims=(tdim, zdim, ydim, xdim))\n",
    "\n",
    "    # rechunk the result\n",
    "    xout = xout.chunk({tdim: 1, zdim: nlev, ydim: ny, xdim: nx})\n",
    "\n",
    "    return xout\n",
    "\n",
    "def flood_kara_xr(dataarray, spval=1e+15):\n",
    "    \"\"\"Apply flood_kara on a xarray.dataarray\n",
    "    Arguments:\n",
    "        dataarray {xarray.DataArray} -- input 2d data array\n",
    "    Keyword Arguments:\n",
    "        spval {float} -- missing value (default: {1e+15})\n",
    "    Returns:\n",
    "        numpy.ndarray -- field after extrapolation\n",
    "    \"\"\"\n",
    "\n",
    "    masked_array = dataarray.squeeze().to_masked_array()\n",
    "    out = flood_kara_ma(masked_array, spval=spval)\n",
    "    return out\n",
    "\n",
    "def flood_kara_ma(masked_array, spval=1e+15):\n",
    "    \"\"\"Apply flood_kara on a numpy masked array\n",
    "    Arguments:\n",
    "        masked_array {np.ma.masked_array} -- array to extrapolate\n",
    "    Keyword Arguments:\n",
    "        spval {float} -- missing value (default: {1e+15})\n",
    "    Returns:\n",
    "        out -- field after extrapolation\n",
    "    \"\"\"\n",
    "\n",
    "    field = masked_array.data\n",
    "\n",
    "    if np.isnan(field).all():\n",
    "        # all the values are NaN, can't do anything\n",
    "        out = field.copy()\n",
    "    else:\n",
    "        # proceed with extrapolation\n",
    "        field[np.isnan(field)] = spval\n",
    "        mask = np.ones(field.shape)\n",
    "        mask[masked_array.mask] = 0\n",
    "        out = flood_kara_raw(field, mask)\n",
    "    return out\n",
    "\n",
    "\n",
    "def flood_kara_raw(field, mask, nmax=1000):\n",
    "    \"\"\"Extrapolate land values onto land using the kara method\n",
    "    (https://doi.org/10.1175/JPO2984.1)\n",
    "    Arguments:\n",
    "        field {np.ndarray} -- field to extrapolate\n",
    "        mask {np.ndarray} -- land/sea binary mask (0/1)\n",
    "    Keyword Arguments:\n",
    "        nmax {int} -- max number of iteration (default: {1000})\n",
    "    Returns:\n",
    "        drowned -- field after extrapolation\n",
    "    \"\"\"\n",
    "\n",
    "    ny, nx = field.shape\n",
    "    nxy = nx * ny\n",
    "    # create fields with halos\n",
    "    ztmp = np.zeros((ny+2, nx+2))\n",
    "    zmask = np.zeros((ny+2, nx+2))\n",
    "    # init the values\n",
    "    ztmp[1:-1, 1:-1] = field.copy()\n",
    "    zmask[1:-1, 1:-1] = mask.copy()\n",
    "\n",
    "    ztmp_new = ztmp.copy()\n",
    "    zmask_new = zmask.copy()\n",
    "    #\n",
    "    nt = 0\n",
    "    while (zmask[1:-1, 1:-1].sum() < nxy) and (nt < nmax):\n",
    "        for jj in np.arange(1, ny+1):\n",
    "            for ji in np.arange(1, nx+1):\n",
    "\n",
    "                # compute once those indexes\n",
    "                jjm1 = jj-1\n",
    "                jjp1 = jj+1\n",
    "                jim1 = ji-1\n",
    "                jip1 = ji+1\n",
    "\n",
    "                if (zmask[jj, ji] == 0):\n",
    "                    c6 = 1 * zmask[jjm1, jim1]\n",
    "                    c7 = 2 * zmask[jjm1, ji]\n",
    "                    c8 = 1 * zmask[jjm1, jip1]\n",
    "\n",
    "                    c4 = 2 * zmask[jj, jim1]\n",
    "                    c5 = 2 * zmask[jj, jip1]\n",
    "\n",
    "                    c1 = 1 * zmask[jjp1, jim1]\n",
    "                    c2 = 2 * zmask[jjp1, ji]\n",
    "                    c3 = 1 * zmask[jjp1, jip1]\n",
    "\n",
    "                    ctot = c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8\n",
    "\n",
    "                    if (ctot >= 3):\n",
    "                        # compute the new value for this point\n",
    "                        zval = (c6 * ztmp[jjm1, jim1] +\n",
    "                                c7 * ztmp[jjm1, ji] +\n",
    "                                c8 * ztmp[jjm1, jip1] +\n",
    "                                c4 * ztmp[jj, jim1] +\n",
    "                                c5 * ztmp[jj, jip1] +\n",
    "                                c1 * ztmp[jjp1, jim1] +\n",
    "                                c2 * ztmp[jjp1, ji] +\n",
    "                                c3 * ztmp[jjp1, jip1]) / ctot\n",
    "\n",
    "                        # update value in field array\n",
    "                        ztmp_new[jj, ji] = zval\n",
    "                        # set the mask to sea\n",
    "                        zmask_new[jj, ji] = 1\n",
    "        nt += 1\n",
    "        ztmp = ztmp_new.copy()\n",
    "        zmask = zmask_new.copy()\n",
    "\n",
    "        if nt == nmax:\n",
    "            raise ValueError('number of iterations exceeded maximum, '\n",
    "                             'try increasing nmax')\n",
    "\n",
    "    drowned = ztmp[1:-1, 1:-1]\n",
    "\n",
    "    return drowned\n",
    "\n",
    "def vgrid_to_interfaces(vgrid, max_depth=6500.0):\n",
    "    \"\"\"Convert layer thicknesses to interface depths.\n",
    "    Args:\n",
    "        vgrid: array of layer thicknesses.\n",
    "        max_depth: maximum depth of the model. The lowest interface depth will be set to this.\n",
    "    Returns:\n",
    "        Array of interface depths.     \n",
    "    \"\"\"\n",
    "    if isinstance(vgrid, xarray.DataArray):\n",
    "        vgrid = vgrid.data\n",
    "    zi = np.concatenate([[0], np.cumsum(vgrid)])\n",
    "    zi[-1] = max_depth\n",
    "    return zi\n",
    "\n",
    "\n",
    "def vgrid_to_layers(vgrid, max_depth=6500.0):\n",
    "    \"\"\"Convert layer thicknesses to depths of layer midpoints.\n",
    "    Args:\n",
    "        vgrid: array of layer thicknesses.\n",
    "        max_depth: maximum depth of the model. The lowest interface depth will be set to this.\n",
    "    Returns:\n",
    "        Array of layer depths.     \n",
    "    \"\"\"\n",
    "    if isinstance(vgrid, xarray.DataArray):\n",
    "        vgrid = vgrid.data\n",
    "    ints = vgrid_to_interfaces(vgrid, max_depth=max_depth)\n",
    "    z = (ints + np.roll(ints, shift=1)) / 2\n",
    "    layers = z[1:]\n",
    "    return layers\n",
    "\n",
    "\n",
    "def rotate_uv(u, v, angle, in_degrees=False):\n",
    "    \"\"\"Rotate velocities from earth-relative to model-relative.\n",
    "    Args:\n",
    "        u: west-east component of velocity.\n",
    "        v: south-north component of velocity.\n",
    "        angle: angle of rotation from true north to model north.\n",
    "        in_degrees (bool): typically angle is in radians, but set this to True if it is in degrees.\n",
    "    Returns:\n",
    "        Model-relative west-east and south-north components of velocity.\n",
    "    \"\"\"\n",
    "    if in_degrees:\n",
    "        angle = np.radians(angle)\n",
    "    urot = np.cos(angle) * u + np.sin(angle) * v\n",
    "    vrot = -np.sin(angle) * u + np.cos(angle) * v\n",
    "    return urot, vrot\n",
    "\n",
    "\n",
    "def interpolate_flood_tracers(ds, target_grid):\n",
    "    \"\"\"Interpolate and flood data at tracer points (temperature, salinity, free surface).\n",
    "    Args:\n",
    "        ds (xarray.Dataset): Dataset with variables temp, salt, and ssh.\n",
    "        target_grid (xarray.Dataset): Model supergrid with variables x, y and coords nxp, nyp.\n",
    "    Returns:\n",
    "        xarray.Dataset: Dataset flooded and interpolated to MOM tracer grid. \n",
    "    \"\"\"\n",
    "    # Flood temperature and salinity over land.\n",
    "    flooded = xarray.merge((\n",
    "        flood_kara(ds[v], zdim='zl') for v in ['temp', 'salt']\n",
    "    ))\n",
    "    \n",
    "    # Flood ssh separately to avoid extra z=0\n",
    "    flooded['ssh'] = flood_kara(ds['ssh']).isel(z=0).drop('z')\n",
    "    \n",
    "    # Interpolate\n",
    "    target_points = (\n",
    "        target_grid\n",
    "        [['x', 'y']]\n",
    "        .isel(nxp=slice(1, None, 2), nyp=slice(1, None, 2))\n",
    "        .rename({'y': 'lat', 'x': 'lon', 'nxp': 'xh', 'nyp': 'yh'})\n",
    "    )\n",
    "    soda_to_mom = xesmf.Regridder(\n",
    "        flooded, \n",
    "        target_points, \n",
    "        method='bilinear', \n",
    "        filename='regrid_soda_tracers.nc',\n",
    "        reuse_weights=False,\n",
    "        periodic=True\n",
    "    )\n",
    "    interped = soda_to_mom(flooded)#.drop(['lon', 'lat'])\n",
    "    return interped\n",
    "\n",
    "\n",
    "def interpolate_flood_velocity(ds, target_grid):\n",
    "    \"\"\"Interpolate and flood velocity data.\n",
    "    Args:\n",
    "        ds (xarray.Dataset): Dataset with variables u and v.\n",
    "        target_grid (xarray.Dataset): Model supergrid with variables x, y and coords nxp, nyp.\n",
    "    Returns:\n",
    "        xarray.Dataset: Dataset flooded and interpolated to MOM velocity grid. \n",
    "    \"\"\"\n",
    "    # Flood over land.\n",
    "    flooded = xarray.merge((\n",
    "        flood_kara(ds[v], zdim='zl') for v in ['u', 'v']\n",
    "    ))\n",
    "\n",
    "    # Interpolate u and v onto supergrid to make rotation possible\n",
    "    target_uv = (\n",
    "        target_grid\n",
    "        [['x', 'y']]\n",
    "        .rename({'y': 'lat', 'x': 'lon'})\n",
    "    )\n",
    "    soda_to_uv = xesmf.Regridder(\n",
    "        ds, target_uv, \n",
    "        filename='regrid_soda_uv.nc',\n",
    "        method='nearest_s2d',\n",
    "        reuse_weights=False,\n",
    "        periodic=True\n",
    "    )\n",
    "    interped_uv = soda_to_uv(flooded[['u', 'v']])#.drop(['lon', 'lat'])\n",
    "    urot, vrot = rotate_uv(interped_uv['u'], interped_uv['v'], target_grid['angle_dx'])\n",
    "    # Subset onto u and v points.\n",
    "    uo = urot.isel(nxp=slice(0, None, 2), nyp=slice(1, None, 2)).rename({'nxp': 'xq', 'nyp': 'yh'})\n",
    "    uo.name = 'u'\n",
    "    vo = vrot.isel(nxp=slice(1, None, 2), nyp=slice(0, None, 2)).rename({'nxp': 'xh', 'nyp': 'yq'})\n",
    "    vo.name = 'v'\n",
    "    \n",
    "    interped = (\n",
    "        xarray.merge((uo, vo))\n",
    "        .transpose('time', 'zl', 'yh', 'yq', 'xh', 'xq')\n",
    "    )\n",
    "\n",
    "    return interped\n",
    "\n",
    "\n",
    "def write_initial(soda_file, vgrid_file, grid_file, start_date, output_file):\n",
    "    \"\"\"Interpolate initial conditions for MOM from a SODA file and write to a new file.\n",
    "    Args:\n",
    "        soda_file (str): Path to SODA file to use for initial conditions.\n",
    "        vgrid_file (str): Path to vertical grid to interpolate data to.\n",
    "        grid_file (str): Path to horizontal grid file (ocean_hgrid.nc) to interpolate data to.\n",
    "        start_date (np.datetime64): Overwrite the SODA datetime with this datetime. Useful if model start date and SODA 5-day dates do not match.\n",
    "        output_file (str): Write resulting initial conditions to this file.\n",
    "    \"\"\"\n",
    "    # ahve ot load the dataset instead of dataarray for nwa25\n",
    "    vgrid = xarray.open_dataset(vgrid_file)\n",
    "    # our vgrid has depths, so we don't need to run vgrid to layers \n",
    "    z = vgrid['z_l'] #vgrid_to_layers(vgrid)\n",
    "    ztarget = xarray.DataArray(\n",
    "        z,\n",
    "        name='z_l',\n",
    "        dims=['z_l'], \n",
    "        coords={'z_l': z}\n",
    "    )\n",
    "    ztarget = ztarget.rename({'z_l' : 'zl'})\n",
    "\n",
    "    soda = (\n",
    "        xarray.open_dataset(soda_file)\n",
    "        .rename({'st_ocean': 'z'})\n",
    "        [['temp', 'salt', 'ssh', 'u', 'v', 'xt_ocean', 'yt_ocean']]\n",
    "    )\n",
    "\n",
    "    soda = soda.to_dask_dataframe()\n",
    "\n",
    "    for var in list(soda.keys()):\n",
    "        soda[var] = soda[var].astype('uint')\n",
    "\n",
    "    # Interpolate SODA vertically onto target grid.\n",
    "    # Depths below bottom of SODA are filled by extrapolating the deepest available value.\n",
    "    revert = soda.interp(z=ztarget, kwargs={'fill_value': 'extrapolate'}).ffill('zl', limit=None)\n",
    "    for var in list(revert.keys()):\n",
    "        revert[var] = revert[var].astype('uint')\n",
    "        \n",
    "    # Split SODA into data on tracer and velocity points\n",
    "    tracers = revert[['temp', 'salt', 'ssh']].rename({'xt_ocean': 'lon', 'yt_ocean': 'lat'})\n",
    "    for var in list(tracers.keys()):\n",
    "        tracers[var] = tracers[var].astype('uint')\n",
    "        \n",
    "    velocity = revert[['u', 'v']].rename({'xu_ocean': 'lon', 'yu_ocean': 'lat'})\n",
    "    for var in list(tracers.keys()):\n",
    "        tracers[var] = tracers[var].astype('uint')\n",
    "\n",
    "    # Horizontally interpolated the vertically interpolated\n",
    "    # and flooded data onto the MOM grid.\n",
    "    grid = xarray.open_dataset(grid_file)\n",
    "    grid = grid.to_dask_dataframe()\n",
    "\n",
    "    interped = xarray.merge((\n",
    "        interpolate_flood_tracers(tracers, grid),\n",
    "        interpolate_flood_velocity(velocity, grid)\n",
    "    ))\n",
    "\n",
    "    # Overwrite the SODA file time with the intended model start date.\n",
    "    interped['time'] = (('time', ), [start_date])\n",
    "\n",
    "    # Fix output metadata, including removing all _FillValues.\n",
    "    all_vars = list(interped.data_vars.keys()) + list(interped.coords.keys())\n",
    "    encodings = {v: {'_FillValue': None} for v in all_vars}\n",
    "    encodings['time'].update({'dtype':'uint8', 'calendar': 'gregorian'})\n",
    "    interped['zl'].attrs = {\n",
    "        'units': 'meter',\n",
    "        'cartesian_axis': 'Z',\n",
    "        'positive': 'down'\n",
    "    }\n",
    "\n",
    "    interped.to_netcdf(\n",
    "        output_file,\n",
    "        format='NETCDF3_64BIT',\n",
    "        engine='netcdf4',\n",
    "        encoding=encodings,\n",
    "        unlimited_dims='time'\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed065f-ad63-4468-aade-90f7edac92c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
